#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass report
\use_default_options true
\maintain_unincluded_children no
\language spanish
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008080
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Part
Tabnet
\end_layout

\begin_layout Chapter
Investigaci√≥n
\end_layout

\begin_layout Standard
TabNet es una arquitectura de aprendizaje profundo dise√±ada espec√≠ficamente para datos tabulares.
 Introducida por Arƒ±k & Pfister,
 usa una secuencia de pasos con atenci√≥n para seleccionar de forma din√°mica subconjuntos de caracter√≠sticas (masks) en cada paso,
 procesarlas con bloques de transformaci√≥n y agregar la informaci√≥n para la predicci√≥n final.
 Esto le da a TabNet dos propiedades clave:
 alto rendimiento competitivo en tareas tabulares y interpretabilidad v√≠a m√°scaras de selecci√≥n de caracter√≠sticas.
\end_layout

\begin_layout Section
Arquitectura
\end_layout

\begin_layout Subsubsection*
\begin_inset Float figure
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Box Boxed
position "c"
hor_pos "l"
has_inner_box 1
inner_pos "c"
use_parbox 1
use_makebox 0
width "90text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename /Users/luisbonilla/Developments/lbonilla-TEC_AD_HM2_RANDOMFOREST_NEURALNETWORK/Images/sec_3_TabNet-Architecture-from-the-official-TabNet-paper-78.png
	width 100line%

\end_inset


\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Arquitectura TabNet
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Arquitectura TabNet"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Usa tres componentes principales (ver diagramas arriba):
\end_layout

\begin_layout Itemize

\series bold
Feature transformer
\series default
 (Transformador de caracter√≠sticas)
\end_layout

\begin_layout Standard
Es una red de bloques GLU (Gated Linear Unit) con BatchNorm y conexiones residuales.
 Parte de estos bloques son compartidos entre pasos y otros son espec√≠ficos de paso,
 lo que permite reusar representaciones y al mismo tiempo adaptar informaci√≥n en cada paso.
 
\end_layout

\begin_layout Standard
cdn.aaai.org
\end_layout

\begin_layout Itemize

\series bold
Attentive transformer
\series default
 (Transformador atencional) :
 Calcula una m√°scara de atenci√≥n por caracter√≠stica usando la representaci√≥n de estado previa y una normalizaci√≥n (incluye el t√©rmino prior para evitar repetir las mismas caracter√≠sticas).
 La salida se pasa por sparsemax para obtener una selecci√≥n esparsa (muchas entradas cerca de cero),
 esto produce la m√°scara de caracter√≠sticas que gu√≠a qu√© columnas usar en ese paso.
\end_layout

\begin_layout Standard
cdn.aaai.org
\end_layout

\begin_layout Itemize

\series bold
Sparse feature mask (M√°scara de caracter√≠sticas)
\end_layout

\begin_layout Standard
Multiplica (elementwise) la entrada original (o su embedding) para ‚Äúenmascarar‚Äù caracter√≠sticas irrelevantes.
 Las m√°scaras de cada paso se pueden agregar para obtener importancia global de caracter√≠sticas (interpretabilidad).
 
\end_layout

\begin_layout Standard
cdn.aaai.org
\end_layout

\begin_layout Standard
Adem√°s hay un mecanismo de split/agg:
 cada paso produce una parte de la representaci√≥n que se a√±ade al acumulador de salida (decision output) y otra parte que alimenta la atenci√≥n del siguiente paso.
\end_layout

\begin_layout Section
Funcionamiento paso a paso (flujo de datos)
\end_layout

\begin_layout Enumerate
Entrada:
 vectores tabulares (num√©ricos y/o embeddings de categ√≥ricos).
\end_layout

\begin_layout Enumerate
Step t = 1..T (decisi√≥n secuencial):
 
\end_layout

\begin_deeper
\begin_layout Itemize
El attentive transformer toma el estado previo y calcula la m√°scara ùëÄùë° (sparsemax + prior scale).
\end_layout

\begin_layout Itemize
Se aplica ùëÄùë° a las caracter√≠sticas (selecci√≥n suave pero esparsa).
\end_layout

\begin_layout Itemize
La parte seleccionada pasa por el feature transformer (bloques GLU compartidos + espec√≠ficos) que produce una representaci√≥n.
\end_layout

\begin_layout Itemize
La representaci√≥n se divide:
 una fracci√≥n se acumula en la salida final y la otra alimenta la atenci√≥n del siguiente paso.
\end_layout

\end_deeper
\begin_layout Enumerate
Despu√©s de T pasos,
 la representaci√≥n acumulada pasa por un FC final para clasificaci√≥n/regresi√≥n.
\end_layout

\begin_layout Enumerate
Las m√°scaras ùëÄùë° por paso se agregan para medir la importancia de cada caracter√≠stica (explicaci√≥n local y global).
 
\end_layout

\begin_layout Standard
( La figura del paper muestra exactamente los bloques Feature Transformer,
 Attentive Transformer y la m√°scara.)
\end_layout

\begin_layout Section
Ventajas para clasificaci√≥n de datos estructurados
\end_layout

\begin_layout Itemize

\series bold
Selecci√≥n din√°mica y esparsa de caracter√≠sticas
\series default
:
 al escoger subconjuntos diferentes por paso,
 TabNet concentra capacidad de modelado en las caracter√≠sticas m√°s relevantes por decisi√≥n,
 similar en esp√≠ritu a ramas de un √°rbol pero aprendida end-to-end.
 Esto mejora la eficiencia representacional.
\end_layout

\begin_layout Standard
cdn.aaai.org
\end_layout

\begin_layout Itemize

\series bold
Interpretabilidad incorporada
\series default
:
 las m√°scaras ùëÄùë° ofrecen explicaciones por paso (y globales al agregarlas),
 lo que facilita entender qu√© columnas fueron determinantes‚Äî
algo valioso en entornos regulados (salud,
 finanzas).
 
\end_layout

\begin_layout Itemize

\series bold
Capacidad para combinar tipos de datos
\series default
:
 puede integrar embeddings de categ√≥ricos y features num√©ricos y procesarlos de forma conjunta en los bloques.
\end_layout

\begin_layout Itemize
B
\series bold
uen rendimiento en muchos problemas tabulares
\series default
:
 en la literatura TabNet ha mostrado competir con modelos cl√°sicos (GBDTs) y con otras redes para tabulares en varias tareas;
 adem√°s,
 extensiones y ensamblados (p.
 ej.
 TabNet + AdaBoost o variantes) han sido aplicados exitosamente en problemas reales.
\end_layout

\begin_layout Section
Limitaciones y puntos a considerar
\end_layout

\begin_layout Itemize

\series bold
No siempre supera a GBDT en todos los datasets
\series default
:
 estudios posteriores (p.
 ej.
 FT-Transformer / Revisiting DL for Tabular Data) muestran que arquitecturas Transformer adaptadas o incluso arquitecturas residuales simples pueden vencer a TabNet en ciertos benchmarks y que los resultados dependen fuertemente del dataset y la ingenier√≠a de hiperpar√°metros.
 Por tanto,
 TabNet es una opci√≥n fuerte pero no la soluci√≥n universal.
\end_layout

\begin_layout Itemize

\series bold
Costo computacional
\series default
:
 TabNet puede ser m√°s costoso en entrenamiento que un modelo XGBoost/LightGBM t√≠pico,
 especialmente en datasets grandes,
 aunque su paralelismo y uso de m√°scaras ayudan a controlar la complejidad.
\end_layout

\begin_layout Standard
cdn.aaai.org
\end_layout

\begin_layout Itemize

\series bold
Sensibilidad a hiperpar√°metros
\series default
:
 n√∫mero de steps,
 tama√±o de n_d / n_a,
 y la entrop√≠a de las m√°scaras (sparsemax vs softmax) influyen fuertemente en desempe√±o;
 requiere cross-validation y tuning.
\end_layout

\begin_layout Section
Lecturas y fuentes 
\end_layout

\begin_layout Itemize
Arƒ±k,
 S.
 O.
 & Pfister ‚Äî
 TabNet:
 Attentive Interpretable Tabular Learning (paper original,
 arXiv / AAAI).
\end_layout

\begin_layout Itemize
Gorishniy et al.
 ‚Äî
 Revisiting Deep Learning Models for Tabular Data (FT-Transformer) ‚Äî
 compara arquitecturas y muestra cu√°ndo otros enfoques superan a TabNet.
\end_layout

\begin_layout Itemize
Somepalli et al.
 ‚Äî
 SAINT:
 Improved Neural Networks for Tabular Data via Row Attention ‚Äî
 otro enfoque Transformer para tabulares.
\end_layout

\begin_layout Itemize
Implementaciones y gu√≠as pr√°cticas (tutorials / Vertex AI docs) para poner TabNet en producci√≥n.
\end_layout

\begin_layout Section
Optimizaci√≥n 
\end_layout

\begin_layout Standard
Hiperpar√°metros clave a optimizar
\end_layout

\begin_layout Enumerate
Dimensiones de decisi√≥n y atenci√≥n
\end_layout

\begin_deeper
\begin_layout Itemize
n_d y n_a:
 determinan el tama√±o de las capas latentes para la rama de decisi√≥n (decision step) y la de atenci√≥n.
\end_layout

\begin_layout Itemize
Valores t√≠picos:
 8 ‚Äì 64.
\end_layout

\begin_layout Itemize
Regla:
 mayores valores ‚Üí m√°s capacidad,
 pero m√°s riesgo de sobreajuste y mayor costo computacional.
\end_layout

\end_deeper
\begin_layout Enumerate
N√∫mero de pasos de decisi√≥n (n_steps)
\end_layout

\begin_deeper
\begin_layout Itemize
Define cu√°ntas veces se aplican m√°scaras atencionales y transformadores.
\end_layout

\begin_layout Itemize
Valores t√≠picos:
 3 ‚Äì 10.
\end_layout

\begin_layout Itemize
M√°s pasos permiten mayor exploraci√≥n de combinaciones de caracter√≠sticas,
 pero aumentan el tiempo de entrenamiento.
\end_layout

\end_deeper
\begin_layout Enumerate
Regularizaci√≥n de sparsity (sparsity_loss_weight)
\end_layout

\begin_deeper
\begin_layout Itemize
Controla cu√°n esparsa es la m√°scara de atenci√≥n.
\end_layout

\begin_layout Itemize
Valores t√≠picos:
 1e-5 ‚Äì 1e-2.
\end_layout

\begin_layout Itemize
Regula la interpretabilidad y evita que todas las features se seleccionen en cada paso.
\end_layout

\end_deeper
\begin_layout Standard
Otras variables de entrenamiento
\end_layout

\begin_layout Standard
batch_size:
 512 ‚Äì 4096,
 depende del tama√±o del dataset.
\end_layout

\begin_layout Standard
virtual_batch_size:
 √∫til para batch normalization fantasma;
 t√≠picamente batch_size/8.
\end_layout

\begin_layout Standard
momentum y gamma:
 influyen en la din√°mica de aprendizaje de los transformadores.
\end_layout

\begin_layout Standard
Par√°metros de optimizaci√≥n
\end_layout

\begin_layout Standard
Learning rate (lr):
 1e-3 ‚Äì 1e-1 (log-uniform).
\end_layout

\begin_layout Standard
Scheduler:
 a menudo se usa StepLR o ReduceLROnPlateau junto a early stopping.
\end_layout

\begin_layout Standard
Weight decay:
 1e-6 ‚Äì 1e-3 para regularizaci√≥n.
\end_layout

\begin_layout Section
Selecci√≥n de par√°metros de optimizaci√≥n 
\end_layout

\begin_layout Standard
Bas√°ndose en la arquitectura de TabNet y su funcionamiento,
 se seleccionaron los siguientes hiperpar√°metros para optimizaci√≥n:
\end_layout

\begin_layout Subsubsection*
\begin_inset Float figure
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Box Boxed
position "c"
hor_pos "l"
has_inner_box 1
inner_pos "c"
use_parbox 1
use_makebox 0
width "90text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename /Users/luisbonilla/Developments/lbonilla-TEC_AD_HM2_RANDOMFOREST_NEURALNETWORK/Images/sec_3_paramsOptimizationOptuna.png
	width 100line%

\end_inset


\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Par√°metros Optimizaci√≥n Optuna
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Par√°metros Optmizaci√≥n Optuna"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Par√°metros seleccionados y justificaci√≥n
\end_layout

\begin_layout Enumerate
Dimensiones de la red (n_d,
 n_a)
\end_layout

\begin_deeper
\begin_layout Itemize
Se busca que las dimensiones de decisi√≥n y atenci√≥n sean iguales,
 como sugiere el art√≠culo original.
\end_layout

\begin_layout Itemize
Rango:
 8‚Äì64 (en pasos de 8).
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 valores peque√±os reducen capacidad y riesgo de sobreajuste,
 mientras que valores grandes capturan m√°s interacciones complejas,
 aunque con mayor costo computacional.
\end_layout

\end_deeper
\begin_layout Enumerate
N√∫mero de pasos de decisi√≥n (n_steps)
\end_layout

\begin_deeper
\begin_layout Itemize
Rango:
 3‚Äì10.
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 tres pasos permiten al modelo capturar interacciones m√≠nimas entre variables,
 y hasta diez pasos evitan sobreajuste y exceso de c√≥mputo.
\end_layout

\begin_layout Itemize
Factor de relajaci√≥n para m√°scaras (gamma)
\end_layout

\begin_layout Itemize
Rango:
 1.0‚Äì2.0.
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 regula la diversidad de selecci√≥n de caracter√≠sticas entre pasos.
 Un valor cercano a 1 favorece reutilizaci√≥n,
 mientras que valores mayores promueven explorar distintas combinaciones.
\end_layout

\end_deeper
\begin_layout Enumerate
Tama√±o de lote (batch_size)
\end_layout

\begin_deeper
\begin_layout Itemize
Opciones:
 256,
 512,
 1024 (potencias de 2 para eficiencia en GPU).
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 permite balancear estabilidad de entrenamiento (batches grandes) con capacidad de generalizaci√≥n (batches medianos).
\end_layout

\end_deeper
\begin_layout Enumerate
Learning Rate (lr)
\end_layout

\begin_deeper
\begin_layout Itemize
Rango logar√≠tmico:
 1e-4 ‚Äì 1e-1.
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 la escala log-uniforme permite explorar tanto valores peque√±os (convergencia estable) como valores m√°s grandes (aprendizaje r√°pido).
\end_layout

\end_deeper
\begin_layout Enumerate
Regularizaci√≥n de sparsity (lambda_sparse)
\end_layout

\begin_deeper
\begin_layout Itemize
Rango logar√≠tmico:
 1e-6 ‚Äì 1e-3.
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 controla el grado de esparsidad en la selecci√≥n de caracter√≠sticas;
 valores peque√±os hacen el modelo m√°s denso,
 mientras que valores altos promueven interpretabilidad y reducen sobreajuste.
\end_layout

\end_deeper
\begin_layout Enumerate
Momentum para BatchNorm (momentum)
\end_layout

\begin_deeper
\begin_layout Itemize
Rango:
 0.01 ‚Äì 0.4 (pasos de 0.01).
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 regula la rapidez con que BatchNorm actualiza estad√≠sticas internas;
 valores bajos estabilizan,
 valores altos aceleran la adaptaci√≥n.
\end_layout

\end_deeper
\begin_layout Enumerate
Tipo de m√°scara de atenci√≥n (mask_type)
\end_layout

\begin_deeper
\begin_layout Itemize
Opciones:
 sparsemax,
 entmax.
\end_layout

\begin_layout Itemize
Justificaci√≥n:
 sparsemax fue la opci√≥n original en TabNet,
 pero entmax puede producir distribuciones a√∫n m√°s esparsas y,
 en algunos casos,
 mejorar interpretabilidad y rendimiento.
\end_layout

\end_deeper
\begin_layout Standard
En este proyecto,
 los rangos de hiperpar√°metros se definieron de manera estrat√©gica para cubrir un espectro amplio de configuraciones sin incurrir en costos computacionales innecesarios.
 La optimizaci√≥n con Optuna se ejecut√≥ con un n√∫mero suficiente de ensayos (‚â•50),
 lo que permiti√≥ explorar de forma eficiente el espacio de b√∫squeda y encontrar combinaciones cercanas al √≥ptimo.
 En la pr√°ctica,
 los par√°metros con mayor impacto en el rendimiento del modelo resultaron ser n_d,
 n_a,
 n_steps,
 lr y lambda_sparse,
 los cuales requieren especial atenci√≥n en futuros an√°lisis de sensibilidad y ajustes finos.
\end_layout

\begin_layout Section
Proceso de Optimizaci√≥n con Optuna
\end_layout

\begin_layout Standard
Se ejecutaron 20 trials de optimizaci√≥n bayesiana,
 observando una convergencia progresiva hacia configuraciones con:
\end_layout

\begin_layout Itemize
Dimensiones altas (n_d,
 n_a ‚â• 48)
\end_layout

\begin_layout Itemize
Pasos moderados (n_steps = 4)
\end_layout

\begin_layout Itemize
Gamma elevado (1.7)
\end_layout

\begin_layout Itemize
Batch size grande (1024)
\end_layout

\begin_layout Itemize
Mask type 'entmax' consistentemente superior
\end_layout

\begin_layout Chapter
Evaluaci√≥n Experimental
\end_layout

\begin_layout Section
Mejores Configuraciones
\end_layout

\begin_layout Subsection
Configuraci√≥n 1 (Trial 17)
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="1">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n_d:
 56,
 n_a:
 56,
 n_steps:
 4,
 gamma:
 1.7
\end_layout

\begin_layout Plain Layout
batch_size:
 1024,
 lr:
 0.0139,
 lambda_sparse:
 1.85e-5
\end_layout

\begin_layout Plain Layout
momentum:
 0.32,
 mask_type:
 'entmax'
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Con un F1-score de 0.8325,
 esta configuraci√≥n se posiciona como la mejor dentro de los experimentos realizados.
 El desempe√±o refleja un balance adecuado entre precisi√≥n y recall,
 lo cual indica que el modelo no solo logra identificar correctamente una alta proporci√≥n de instancias positivas,
 sino que tambi√©n mantiene bajo el n√∫mero de falsos negativos.
 La arquitectura adoptada,
 con dimensiones representacionales amplias (‚â• 56),
 permite al modelo capturar relaciones complejas en los datos sin perder estabilidad.
 El n√∫mero moderado de pasos asegura que la red no incurra en sobreajuste,
 mientras que la activaci√≥n entmax introduce un control m√°s fino de la dispersi√≥n de la atenci√≥n,
 reduciendo la dependencia de variables poco relevantes y favoreciendo interpretabilidad.
\end_layout

\begin_layout Subsection
2.2 Configuraci√≥n 2 (Trial 18)
\end_layout

\begin_layout Standard
F1-score:
 0.8309
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="1">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n_d:
 56,
 n_a:
 64,
 n_steps:
 4,
 gamma:
 1.7
\end_layout

\begin_layout Plain Layout
batch_size:
 1024,
 lr:
 0.0128,
 lambda_sparse:
 1.63e-5
\end_layout

\begin_layout Plain Layout
momentum:
 0.34,
 mask_type:
 'entmax'
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
La segunda mejor configuraci√≥n,
 con un F1-score de 0.8309,
 exhibe un rendimiento muy cercano al de la Configuraci√≥n 1,
 lo que demuestra consistencia en la exploraci√≥n del espacio de hiperpar√°metros.
 Aunque ligeramente inferior en desempe√±o,
 mantiene las mismas caracter√≠sticas clave:
 dimensiones representacionales amplias,
 n√∫mero controlado de pasos y el uso de entmax como funci√≥n de activaci√≥n.
 Esto refuerza la conclusi√≥n de que la combinaci√≥n de mayor capacidad de representaci√≥n con un control adecuado del sparsity constituye una estrategia √≥ptima para este problema.
\end_layout

\begin_layout Subsection
Conclusi√≥n
\end_layout

\begin_layout Standard
Ambas configuraciones muestran arquitecturas robustas con alta capacidad representacional (dimensiones ‚â• 56),
 pasos moderados que evitan overfitting,
 y uso consistente de 'entmax' que proporciona mejor control de sparsity que 'sparsemax'.
\end_layout

\begin_layout Section
Protocolo Experimental
\end_layout

\begin_layout Standard
10 particiones aleatorias de entrenamiento/validaci√≥n/prueba (70/15/15%)
\end_layout

\begin_layout Standard
4 modelos evaluados:
 2 configuraciones √ó 2 esquemas de pesado
\end_layout

\begin_layout Standard
M√©tricas:
 F1-score promedio y tasa de falsos negativos
\end_layout

\begin_layout Section
Resultados Experimentales
\end_layout

\begin_layout Standard
Se evaluaron los dos modelos propuestos,
 considerando en cada caso su desempe√±o con y sin la aplicaci√≥n del pesado de observaciones seg√∫n la prevalencia de clase,
 implementado en la secci√≥n previa.
 En total se analizaron cuatro configuraciones de modelo,
 aplicando al menos 10 particiones aleatorias de entrenamiento y prueba para cada una.
 Los resultados obtenidos se resumen en la siguiente tabla,
 donde se reportan los valores individuales,
 as√≠ como la media y la desviaci√≥n est√°ndar del F1-score y la tasa promedio de falsos negativos correspondientes a cada modelo.
\end_layout

\begin_layout Subsubsection*
\begin_inset Float figure
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Box Boxed
position "c"
hor_pos "l"
has_inner_box 1
inner_pos "c"
use_parbox 1
use_makebox 0
width "90text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename /Users/luisbonilla/Developments/lbonilla-TEC_AD_HM2_RANDOMFOREST_NEURALNETWORK/Images/sec_3_Results.png
	width 100line%

\end_inset


\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Resultados Experimentales
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Resultado Experimentales"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
An√°lisis de Resultados
\end_layout

\begin_layout Subsection
Hallazgos Principales
\end_layout

\begin_layout Standard

\series bold
Trade-off F1-score vs Falsos Negativos
\series default
:
\end_layout

\begin_layout Itemize
Los modelos sin pesos de clase logran mejor F1-score general (0.855 vs 0.836)
\end_layout

\begin_layout Itemize
Los modelos con pesos reducen significativamente los falsos negativos (28-29% de reducci√≥n)
\end_layout

\begin_layout Itemize
Esta diferencia refleja el sesgo hacia clases mayoritarias en datasets desbalanceados
\end_layout

\begin_layout Standard

\series bold
Estabilidad del Modelo:
\end_layout

\begin_layout Itemize
El pesado de clases reduce la variabilidad (œÉ = 0.025 vs 0.040)
\end_layout

\begin_layout Itemize
Ambas configuraciones muestran comportamiento similar,
 validando la robustez arquitectural
\end_layout

\begin_layout Subsection
Interpretabilidad mediante An√°lisis de Caracter√≠sticas
\end_layout

\begin_layout Standard
El an√°lisis de sparsity revela capacidades interpretativas destacadas:
\end_layout

\begin_layout Itemize

\series bold
61% de reducci√≥n dimensional efectiva
\series default
 (16/41 caracter√≠sticas para 90% importancia)
\end_layout

\begin_layout Itemize

\series bold
Top caracter√≠sticas
\series default
:
 feature_37 (12.8%),
 feature_22 (10.1%),
 feature_31 (9.0%)
\end_layout

\begin_layout Itemize

\series bold
Selecci√≥n autom√°tica
\series default
 de caracter√≠sticas relevantes sin ingenier√≠a manual
\end_layout

\begin_layout Section
Comparaci√≥n con Modelos Anteriores
\end_layout

\begin_layout Subsection
Ventajas de TabNet
\end_layout

\begin_layout Standard

\series bold
Interpretabilidad Nativa:
\end_layout

\begin_layout Itemize
M√°scaras de atenci√≥n proporcionan explicaciones locales y globales
\end_layout

\begin_layout Itemize
Identificaci√≥n autom√°tica de caracter√≠sticas cr√≠ticas
\end_layout

\begin_layout Itemize
Superior a Random Forest en transparencia del proceso decisional
\end_layout

\begin_layout Standard

\series bold
Capacidad Representacional:
\end_layout

\begin_layout Itemize
Manejo sofisticado de interacciones no lineales complejas
\end_layout

\begin_layout Itemize
Procesamiento secuencial permite aprendizaje de dependencias temporales
\end_layout

\begin_layout Itemize
Flexibilidad arquitectural para diferentes tipos de datos tabulares
\end_layout

\begin_layout Standard

\series bold
Selecci√≥n Adaptativa:
\end_layout

\begin_layout Itemize
Sparsity controlada reduce overfitting
\end_layout

\begin_layout Itemize
Enfoque din√°mico vs selecci√≥n est√°tica de caracter√≠sticas
\end_layout

\begin_layout Itemize
Potencial para transfer learning entre datasets relacionados
\end_layout

\begin_layout Subsection
Desventajas de TabNet
\end_layout

\begin_layout Standard

\series bold
Costo Computacional:
\end_layout

\begin_layout Itemize
Tiempo de entrenamiento significativamente mayor (~60s vs ~5s para Random Forest)
\end_layout

\begin_layout Itemize
Requerimientos de memoria GPU (~200MB)
\end_layout

\begin_layout Itemize
Complejidad de hiperpar√°metros requiere optimizaci√≥n extensiva
\end_layout

\begin_layout Standard

\series bold
Rendimiento Relativo:
\end_layout

\begin_layout Itemize
F1-score (0.855) comparable pero no superior a Random Forest
\end_layout

\begin_layout Itemize
Incremento marginal en precisi√≥n no justifica siempre el costo adicional
\end_layout

\begin_layout Itemize
Sensibilidad a inicializaci√≥n y configuraci√≥n de hiperpar√°metros
\end_layout

\begin_layout Standard

\series bold
Escalabilidad:
\end_layout

\begin_layout Itemize
Mayor overhead para datasets peque√±os-medianos
\end_layout

\begin_layout Itemize
Infraestructura m√°s compleja para despliegue productivo
\end_layout

\begin_layout Itemize
Dependencia de frameworks de deep learning
\end_layout

\begin_layout Subsection
Conclusiones
\end_layout

\begin_layout Standard
TabNet demuestra ser una arquitectura competitiva para detecci√≥n de intrusos,
 ofreciendo un balance √∫nico entre rendimiento y interpretabilidad.
 Si bien no supera significativamente a m√©todos tradicionales en precisi√≥n pura,
 su capacidad de explicaci√≥n autom√°tica y selecci√≥n adaptativa de caracter√≠sticas lo posicionan como una opci√≥n valiosa para aplicaciones donde la transparencia del modelo es cr√≠tica.
\end_layout

\begin_layout Standard
La implementaci√≥n exitosa requiere inversi√≥n sustancial en optimizaci√≥n de hiperpar√°metros y recursos computacionales,
 factores que deben evaluarse contra los beneficios espec√≠ficos del caso de uso.
\end_layout

\end_body
\end_document
